---
title: 'Estudo de caso: Grupo D 3'
author: "Gilmar Pereira, Maressa Tavares e Victor Ruela"
date: "29 de Outubro, 2019"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}

bibliography: refsCS03.bib
csl: ieee.csl
---


```{r setup,results='hide',warning=FALSE,echo=FALSE, message=FALSE, include=FALSE}
# A few initial definitions just to make sure all required packages are installed. Change as needed.
# NOTE: It may echo some weird messages to the PDF on the first compile (package installation messages). Run twice and the problem will (hopefully) go away.
if (!require(ggplot2, quietly = TRUE)){
      install.packages("ggplot2")
      }
if (!require(devtools, quietly = TRUE)){
      install.packages("devtools")
      }
 if (!require(broom, quietly = TRUE)){
       devtools::install_github("dgrtwo/broom")
      }
if (!require(GGally, quietly = TRUE)){
      install.packages("GGally")
}

if (!require(ExpDE, quietly = TRUE)){
  install.packages("ExpDE")
}

if (!require(car, quietly = TRUE)){
  install.packages("car")
}

if (!require(boot, quietly = TRUE)){
  install.packages("boot")
}

if (!require(dplyr, quietly = TRUE)){
  install.packages("boot")
}

if (!require(pwr, quietly = TRUE)){
  install.packages("pwr")
}

if (!require(smoof, quietly = TRUE)){
  install.packages("smoof")
}

if (!require(CAISEr, quietly = TRUE)){
  install.packages("CAISEr")
}

if (!require(knitr, quietly = TRUE)){
  install.packages("knitr")
}

if (!require(ggridges, quietly = TRUE)){
  install.packages("ggridges")
}

```
# Summary

O presente trabalho realizou o delineamento e executou os testes estatísticos para avaliar o desempenho médio do algoritmo conhecido como Evoluçao Diferencial [@storn1997differential]. O algoritmo foi desenvolvido no ano de 1997 por Storn e Price e é um algoritmo simples de otimização multimodal, primeiramente desenvolvido para otimização de funções contínuas e variáveis numéricas discretas [@price2013differential].  

Para o presente trabalho o algoritmo DE (Differential Evolution) foi ajustado com duas configurações alterando a forma de recombinação e mutação nos algoritmos. As classes de funções para este experimento foi composta pela função Rosenbrock [@rosenbrock1960automatic] de dimensões entre 2 e 150. Para  analisar os dados utilizou-se da técnica de blocagem determinando a quantidade de blocos e seus tamanhos, assim como o número de amostras por instância.

Realizou-se o cálculo do número de blocos de acordo com [@Campelo2018] e o número de instâncias de acordo com [@articlefcampelo]. Para teste das premissas de normalidade utilizou-se a ferramenta qqplot e o teste de shapiro-wilk. O teste F foi utilizado para avaliar a homocedasticidade, e por fim, para comparar os dois algoritmos utilizou-se o teste não paramétrico de Friedman, tendo em vista não validação das premissas da ANOVA.

# Planejamento do Experimento

Nesta seção é apresentado o planejamento do experimento, descrevendo os objetivos e o delineamento do experimento.

## Objetivo do Experimento

O objetivo deste experimento é analisar se exite alguma diferença entre duas configuração do algoritmo DE dentre as classes de funções, determinando a configuração de melhor desempenho e ressaltando as magnitudes das diferenças encontradas.

## Delineamento

Para o seguinte experimento foram realizadas as seguintes etapas, as quais estão detalhadas nas próximas seções. 

* Formulação das hipóteses de teste; 
* Cálculo dos tamanhos amostrais, estabelecimento das quantidades de instâncias e número de iterações do algoritmo; 
* Coleta e tabulação dos dados, 
* Realização dos testes de hipóteses; 
* Estimação da magnitude das diferenças;
* Validação das premissas; 
* Resultados e Conclusões.

## Hipóteses

Para a análise comparativa entre as configurações do algoritmo DE, determinou-se as seguintes hipóteses a serem testadas.
$$\begin{cases} H_0: \mu_{1} = \mu_{2}&\\H_1: \mu_{1} \neq \mu_{2}\end{cases}$$
Onde $\mu_{1}$ e $\mu_{2}$ são as médias amostrais das configurações 1 e 2 dos algoritmos, respectivamente. 

Além disso, foram definidos os seguintes parâmetros experimentais: 

* Significância desejada: $\alpha = 0.05$.
* Mínima diferença de importância prática (padronizada): $d^* = \delta^*/\sigma = 0.5$
* Potência mínima desejada $\pi = 1 - \beta = 0.8$

## Coleta dos Dados

Neste trabalho, cada amostra consiste em uma execução do algoritmo DE, para cada instância (dimensão da função objetivo) e configuração do algoritmo em questão (níveis). Foram escolhidas $N = 30$ repetições de cada par instância-configuração, recomendado como suficiente por Campelo e Takahashi [@articlefcampelo]. A coleta de dados foi dividida em duas etapas, descritas nas seções a seguir. O código utilizado para a coleta de dados está disponível no apêndice deste trabalho.

### Geração do arquivo de configuração do experimento

Esta etapa permite gerar um arquivo .csv contendo a configuração descrita a seguir. As rotinas foram implementadas de forma a possibilitar a criação da configuração para quaisquer número de repetições ($N$), instâncias ($I$), grupos ($b$) e níveis ($a$). Um último passo consiste em randomizar o arquivo de configuração e dividí-lo em 3 arquivos separados, a serem executados por cada membro do grupo. Isso a independência das amostras geradas e que o experimento seja completamente randomizado. Como o algoritmo demora um tempo considerável para sua execução, a divisão entre os participantes permitiu a sua execução em paralelo para otimizar o tempo necessário para gerar todos os dados. A tabela abaixo exibe um exemplo de arquivo de configuração gerado.

```{r config_file, echo=FALSE}
fname = 'rcbd.config.victor.csv'
Z <- read.csv(fname)
Z[,"result"] = -1
kable(Z[5:9, 1:6], caption = 'Exemplo de arquivo de configuração', align = 'l')
```

### Execução de arquivo de configuração

Com o arquivo de configuração disponível, o experimento está pronto para ser executado. A rotina desenvolvida carrega um arquivo de configuração (.csv) e executa cada linha em sequência, para os seus respectivos parâmetros. À medida em que uma amostra é finalizada, o resultado é salvo no próprio arquivo de configuração, na coluna `result`. Isso garante que seja possível continuar a execução do arquivo sem perder as amostras realizadas anteriormente, caso ocorra algum problema.

## Análise Exploratória dos Dados

Nesta seção é apresentado uma análise exploratória dos dados, verificando normalidade, homocedasticidade, independência, que são as premissas que devem ser validadas antes da realização dos testes estatísticos.

Como o estudo consiste na comparação entre os resultados da execução de duas configurações de um algoritmo de otimização, a dimensão da função objetivo é um fator importante. Logo, a análise exploratória foi feita considerando amostras de instâncias de baixa, média e alta dimensão. Inicialmente, os dados do experimento foram carregados, sendo as instâncias 4, 50 e 100 escolhidas para avaliação, e um gráfico boxplot foi criado para a análise preliminar.

```{r read_data}
sample.all <- read.csv('data.all.instances.csv', header = TRUE)
sample.all$configuration <- as.factor(sample.all$algorithm)
sample.all <- sample.all %>% mutate(logresult = log(result))

sample.all.eda <- sample.all %>% filter(instance == 100 | instance == 50 | instance == 4)
```
```{r plot_boxplot, echo=FALSE, fig.width=7,fig.height=4, fig.cap="Boxplot dos dados\\label{fig:boxplot}"}
ggplot(sample.all.eda, aes(x=configuration, y=result)) + 
  geom_boxplot() +  facet_grid(rows = vars(instance), scales = "free_y")
```
Através deste gráfico, as seguintes observações podem ser feitas:

* Os valores da função objetivo final possuem magnitudes muito diferentes dependendo da dimensão. Portanto, uma normalização dos dados para uma escala comum pode ser necessária para a correta análise dos experimentos.
* Há algumas repetições do algoritmo que poderiam ser considerados outliers. Elas devem ser removidas de forma a não prejudicar os testes de hipótese e validação das premissas.
* A configuração 2 parece obter melhores resultados para dimensões baixas, quase sempre chegando ao mínimo da função. Entretnato, o mesmo não pode ser afirmado para dimensões maiores.

## Cálculo do número de blocos

De acordo com [@Campelo2018], o número de blocos ideal é calculado variando a quantidade de blocos enquanto a relação

$$F(1-\alpha) \leq F(\beta,\phi)$$
é respeitada. Onde $\phi$ é o parâmetro de não-centralidade, definido por:

$$\phi = \frac{b \sum^{a}_{i=1}\tau_i}{a\sigma^2}$$
De acordo com a definição do experimento, temos que $a = 2$, tamanho de efeito normalizado $d = 0.5$, potência desejada de $\pi = 0.8$ e significância $\alpha = 0.05$. Logo, é possível calcular o número de blocos $b$ de acordo com a rotina abaixo.

```{r blockcalculation, echo = TRUE}
a <- 2
d <- 0.5
alpha <- 0.05
beta <- 0.2

tau <- c(-d, d, rep(0, a - 2)) # define tau vector
b <- 5

tb <- data.frame(b = rep(-1, 50), ratio = rep(-1,50), phi = rep(-1,50))

for(i in seq(1,40,by=2)){

  b <- i + 5
  f1 <- qf(1 - alpha, a - 1, (a - 1)*(b - 1))
  f2 <- qf(beta, a - 1, (a - 1)*(b - 1), (b*sum(tau^2)/a))
  phi <- b*sum(tau^2)/a
  
  tb[i, ] = c(b, f1/f2, phi)
}

```

Portanto, o número mínimo de blocos necessários é de `r tb %>% filter(ratio <= 1 & ratio > 0) %>% select(b) %>% min()`. As iterações podem ser vistas na tabela abaixo.

```{r tabledisplay, echo = FALSE}
kable(tb[seq(1,40,by=4),] %>% filter(b > 0), col.names = c("Blocos","Razão","Phi"), caption = 'Iterações para cálculo do número de blocos', align = 'l')
```

Portanto, devemos escolher $b$ blocos aleatoriamente das instâncias disponíveis. A figura \ref{fig:ridge} exibe o gráfico de ridge para as instâncias selecionadas. 

```{r block_instances, echo=FALSE, fig.width=6,fig.height=3.5, fig.cap="Ridge plot dos dados\\label{fig:ridge}"}
data.all.instances.log <- sample.all %>% group_by(instance) %>% 
  mutate(Max = max(result), Min = min(result), Std = sd(result), Avg = mean(result)) %>% 
  mutate(resultLog = log(result))

set.seed(1234)
random.blocks <- sample(5:150, b)
data.block.instances <- data.all.instances.log %>%  filter(instance %in% random.blocks)

data.block.instances$instance = as.factor(data.block.instances$instance)
data.block.instances$algorithm = as.factor(data.block.instances$algorithm)

ggplot(data.block.instances, aes(x = instance, 
                            y = resultLog, 
                            group = (algorithm), 
                            colour = (algorithm))) + geom_line(linetype=2) + geom_point(size=2)


ggplot(data.block.instances, aes(x = resultLog, y = instance, fill = algorithm)) + geom_density_ridges() +
  geom_density_ridges(scale = 10, size = 0.25, rel_min_height = 0.03) + theme(legend.position="bottom")

```

Pela análise das figuras percebe-se que as duas configurações do algoritmo possuem um comportamento semelhanto. A exceção é para as instâncias inferiores, nas quais percebe-se um melhor desempenho do algoritmo 2.

## Validação das premissas

Para realizar as inferências estatísticas sobre as duas configurações do algoritmo de otimização é necessário validar as premissas antes de executar o teste. Neste caso, como tratam-se de duas configurações em um espectro amplo de dimensões, existe um fator conhecido e controlável que pode influenciar no resultado do teste. Então, para eliminar o efeito desse fator indesejável uma opção é realizar a blocagem [@montgomery2007applied]. A seguir são apresentados os testes realizados para validar as premissas exigidas pela blocagem (ANOVA).

A - Normalidade

Para a validação desta premissa, aplicou-se o teste ANOVA aos dados e depois foram avaliados os resíduos obtidos pelo moddelo. O gráfico quantil-quantil e teste de shapiro-wilk também foram utilizados nessa validação, como apresentados a seguir.

```{r res_blocks, echo=FALSE, fig.width=7,fig.height=4, fig.cap="QQ-plot dos resíduos \\label{fig:qqres}"}
res.aov <- aov(resultLog  ~ algorithm + instance, data = data.block.instances)
summary(res.aov)
par(mfrow=c(2,2))
plot(res.aov)
#invisible(qqPlot(res.aov$residuals, dist='norm',envelope=.95, las = 1, pch = 16))
shapiro.test(res.aov$residuals)
```

De acordo o p-valor (2.2e-16), conclui-se que não há evidências de normalidade dos resíduos.

Para se ter uma ideia inicial da normalidade dos dados, o histograma para as instâncias em avaliação é gerado a seguir.

```{r plot_histogram, echo=FALSE, fig.width=8,fig.height=5, fig.cap="Histograma dos dados\\label{fig:histogram}"}
ggplot(sample.all.eda, aes(fill=configuration, x=result)) + 
  geom_histogram(bins=30) +  facet_grid(cols = vars(instance), scales = "free_x") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Pelo histograma apresentado, é possível notar que os dados não apresentam, visivelmente, uma distribuição  normal. Assim, os gráficos quantil-quantil e os testes de Shapiro-Wilk foram executados para verificar essa observação.

```{r plot_norm_dim4, echo=FALSE, fig.width=7,fig.height=7, fig.cap="Gráfico quantil-quantil das instâncias avaliadas\\label{fig:norm}"}
sample.all.dim.4.algo1 <- sample.all.eda %>% filter(algorithm == 1 & instance  == 4)
sample.all.dim.4.algo2 <-  sample.all.eda %>% filter(algorithm == 2 & instance == 4)
sample.all.dim.50.algo1 <- sample.all.eda %>% filter(algorithm == 1 & instance == 50)
sample.all.dim.50.algo2 <- sample.all.eda %>% filter(algorithm == 2 & instance == 50)
sample.all.dim.100.algo1 <- sample.all.eda %>% filter(algorithm == 1 & instance == 100)
sample.all.dim.100.algo2 <- sample.all.eda %>% filter(algorithm == 2 & instance == 100)


par(mfrow=c(3,2))
invisible(qqPlot(sample.all.dim.4.algo1$result, dist='norm',envelope=.95, las = 1, pch = 16, main="Configuração 1 - Dimensão 4", ylab = "Result"))

invisible(qqPlot(sample.all.dim.4.algo2$result, dist='norm',envelope=.95, las = 1, pch = 16, main="Configuração 2 - Dimensão 4", ylab = "Result"))

invisible(qqPlot(sample.all.dim.50.algo1$result, dist='norm',envelope=.95, las = 1, pch = 16, main="Configuração 1 - Dimensão 50", ylab = "Result"))

invisible(qqPlot(sample.all.dim.50.algo2$result, dist='norm',envelope=.95, las = 1, pch = 16, main="Configuração 2 - Dimensão 50", ylab = "Result"))

invisible(qqPlot(sample.all.dim.100.algo1$result, dist='norm',envelope=.95, las = 1, pch = 16, main="Configuração 1 - Dimensão 100", ylab = "Result"))

invisible(qqPlot(sample.all.dim.100.algo2$result, dist='norm',envelope=.95, las = 1, pch = 16, main="Configuração 2 - Dimensão 100", ylab = "Result"))

```

```{r shapirotest_all, echo = FALSE}
sw.test.result <- data.frame("config1" = rep(0,b), "config2" = rep(0,b), instance =  rep(0,b))
for (i in seq(1,b)){
  bi <- sort(random.blocks)[i]
  data.block.algo1 <- data.block.instances %>% filter(instance == bi & algorithm == 1)
  data.block.algo2 <- data.block.instances %>% filter(instance == bi & algorithm == 2)
  
  sw.test.result[i,] <- c(shapiro.test(data.block.algo1$result)$p.value,shapiro.test(data.block.algo2$result)$p.value,bi)
}

kable(sw.test.result[,c(3,2,1)], col.names = c("Instância","Configuração 1","Configuração 2"), caption = 'p-valores dos testes de shapiro-wilk \\label{tab:shapiro}', align = 'l')
```

Conforme visto anteriormente, não foi possível atestar a normalidade dos dados. Logo, uma transformação logarítmica é aplicada aos dados e os testes de Shapiro-Wilk foram executados novamente.

```{r shapirotest_log, echo = FALSE}

sw.test.result.log <- data.frame("p.value" = rep(0,6), "configuration" =  rep(0,6), instance =  rep(0,6))

sw.test.result.log[1,] <- c(shapiro.test(sample.all.dim.4.algo1$logresult)$p.value,1,4)
sw.test.result.log[2,] <- c(shapiro.test(sample.all.dim.4.algo2$logresult)$p.value,2,4)
sw.test.result.log[3,] <- c(shapiro.test(sample.all.dim.50.algo1$logresult)$p.value,1,50)
sw.test.result.log[4,] <- c(shapiro.test(sample.all.dim.50.algo2$logresult)$p.value,2,50)
sw.test.result.log[5,] <- c(shapiro.test(sample.all.dim.100.algo1$logresult)$p.value,1,100)
sw.test.result.log[6,] <- c(shapiro.test(sample.all.dim.100.algo2$logresult)$p.value,2,100)

kable(sw.test.result.log[,c(3,2,1)], col.names = c("Instância","Configuração","p-valor"), caption = 'Resultados dos testes de shapiro-wilk com aplicação de transformaçao logarítimica\\label{tab:log_shap}', align = 'l')
```

Pelas tabelas \ref{tab:shapiro} e \ref{tab:log_shap}, é possível notar que após a aplicação da transformação logarítmica, há resultados que deixam de ser normais. Portanto, não podemos afirmar que para todas as dimensões e configurações testadas os dados seguem uma distribuição normal.

B - Igualdade de Variância

Para validação dessa premissa utilizou-se o teste de homogeniedade de variâncias no qual a hipótese nula considera que razão entre as variâncias é igual 1. Primeiramente analisou-se as instâncias de dimensão 4, 50 e 100, verificando as variâncias das duas configurações. Os resultado do teste podem ser visto na tabela \ref{tab:varteste1} .

```{r variance_test, echo = FALSE, fig.cap="\\label{tab:varteste1}"}
var.test.results <- data.frame("p.value" = rep(0,3), instance =  rep(0,3))
var.test.results[1,] <- c(var.test(sample.all.dim.4.algo1$result,sample.all.dim.4.algo2$result)
$p.value, 4)
var.test.results[2,] <- c(var.test(sample.all.dim.50.algo1$result,sample.all.dim.50.algo2$result)
$p.value, 50)
var.test.results[3,] <- c(var.test(sample.all.dim.100.algo1$result,sample.all.dim.100.algo2$result)
$p.value, 100)

kable(var.test.results[,c(2,1)], col.names = c("Instância","p-valor"), caption = 'Resultados dos testes de variância', align = 'l')
```

Verifica-se que a instância de maior dimensão possui um p-valor alto, em relação ao nível de significancia pré-determinado, desta maneira não há evidências para rejeitar a hipótese nula. Por outro lado, as instâncias menores possuem p-valor baixo, logo, não é possível afirmar a homocedasticidade para tais instâncias. Em seguida analisou-se as duas configurações com instâncias geradas de forma aleatória. O resultado por ser visto na tabela \ref{tab:varteste2}.

```{r vatest, echo = FALSE, fig.cap="\\label{tab:varteste2}"}
sw.test.result <- data.frame("p.value" = rep(0,b), instance =  rep(0,b))
for (i in seq(1,b)){
  bi <- sort(random.blocks)[i]
  data.block.algo1 <- data.block.instances %>% filter(instance == bi & algorithm == 1)
  data.block.algo2 <- data.block.instances %>% filter(instance == bi & algorithm == 2)
  
  sw.test.result[i,] <- c(var.test(data.block.algo1$result, data.block.algo2$result)$p.value,bi)
}

kable(sw.test.result[,c(2,1)], col.names = c("Instância","p-valor"), caption = 'p-valores do teste F', align = 'l')
```

Verifica-se que para diferentes valores de instâncias (dimensões) o p-valor é maior que o nível de significancia pré-estabelecido. Desta forma não há evidências para rejeitar a hipótese nula de que as variâncias são iguais. Analisando de forma geral verifica-se que é possível que as variancias entre as duas configurações sejam iguais.

c - INDEPENDÊNCIA

Como descrito anteriormente, a coleta de dados foi meticulosamente planejada de forma a garantir a independência entre as amostras. As características de cada amostras foram geradas e coletadas de modo aleatório, a fim de evnitar qualquer iterção entre a amostra e a instância avaliada. Desse modo, é possível garantir a independância entre as amostras.

# Resultados

Nesta seção são apresentados os resultados realizando os testes de hipoteses e a determinação da melhor configuração juntamente com a estimação das magnitudes das diferenças.

## Teste de Hipótese - não paramétrico

Verificando que os dados gerados pelas duas configurações não são normais e que não há um homegeniedade entre as variâncias, optou-se pela utilização de um teste não paramétrico. O teste utilizado para verificar a igualdade das médias entre as duas configurações foi o teste de Friedman. O teste de Friedman é um teste não paramétrico que generaliza o teste de sinais e possui um poder estatítico modesto para muitas distribuições não normais [@zimmerman1993relative]. O teste Friedman esta descrito abaixo.

```{r levene_test, echo = FALSE}
leveneTest(result ~ algorithm, data = data.block.instances)
leveneTest(result ~ instance, data = data.block.instances)

df <- data.block.instances[order(data.block.instances$algorithm, data.block.instances$instance),]

df_2 <- df %>% filter(algorithm == 1) %>% group_by(instance, algorithm) %>% summarise(result = mean(result))
df_3 <- df %>% filter(algorithm == 2) %>% group_by(instance, algorithm) %>% summarise(result = mean(result))

df_4 <- df_2 %>% bind_rows(df_3)

df_5 <- df_4[order(df_4$instance, df_4$algorithm),] 
friedman.test(result ~ algorithm | instance, data = df_5)
```

Verifica-se pelo teste que o p-valor baixo evidencia a rejeição da hipótese nula de que as configurações possuem médias iguais.

## Estimação das magnitudes das diferenças

```{r teste_fisher }

G <- 34
means_group1 <- data.frame("Group" = rep(0,G), "mean" =  rep(0,G))
means_group2 <- data.frame("Group" = rep(0,G), "mean" =  rep(0,G))
means_group <- data.frame("Group" = rep(0,G), "mean" =  rep(0,G))
k <- 1
for (i in 1:Group){
  block_i1 <- sample.all %>% filter(algorithm == 1 & group == i)
  block_i2 <- sample.all %>% filter(algorithm == 2 & group == i)
  
  means_group1[i,] <- c(i, mean(block_i1$result))
  means_group2[i,] <- c(i, mean(block_i2$result))
  means_group[k,] <- c(i, sqrt(mean(block_i1$result)^2))
  means_group[k+1,] <- c(i, sqrt(mean(block_i2$result)^2))
  k <- k + 2
}

datas <- matrix(means_group$mean, nrow = 2)

fisher.test(datas)

```

  
## Apêndice
### Geração de configuração
```{r config_generation, eval = FALSE}
# Load packages -----------------------------------------------------------
if (!require(ExpDE, quietly = TRUE)){
  install.packages("ExpDE")
}


if (!require(smoof, quietly = TRUE)){
  install.packages("smoof")
}


if (!require(CAISEr, quietly = TRUE)){
  install.packages("CAISEr")
}


# RCBD functions ----------------------------------------------------
set.seed(15632) # set a random seed
instances <- seq(2, 150) # number of instances
N <- 30 # number of replicates per instance

rcbd.configuration.generator <- function(level, b, instances, N){
  nrows <- length(instances) * N
  n.instances <- length(instances)
  instance <- sort(rep(instances, N))
  groups <- sapply(instance, function(i){ ceiling(i/(n.instances/b))  })
    
  X <- data.frame("algorithm" = rep(level, nrows), 
                  "replicate" = rep(seq(1,N), n.instances),
                  "instance" = instance,
                  "group" =  groups,
                  "result" = rep(-1, nrows))
  return(X)
}


x.config.1 <- rcbd.configuration.generator(1, b, instances, N)
x.config.2 <- rcbd.configuration.generator(2, b, instances, N)
x.config.all <- rbind(x.config.1, x.config.2)
x.config.all.shuffled <- x.config.all[sample(nrow(x.config.all)), ]

split.size <- (nrow(x.config.all.shuffled)/3)
x.config.all.shuffled$member <- ceiling((1:nrow(x.config.all.shuffled))/split.size)

x.config.all.shuffled.victor <- x.config.all.shuffled[x.config.all.shuffled$member == 1,1:5]
x.config.all.shuffled.gilmar <- x.config.all.shuffled[x.config.all.shuffled$member == 2,1:5]
x.config.all.shuffled.maressa <- x.config.all.shuffled[x.config.all.shuffled$member == 3,1:5]

write.csv(x.config.all.shuffled.victor, 'rcbd.config.victor.csv', row.names=FALSE)
write.csv(x.config.all.shuffled.gilmar, 'rcbd.config.gilmar.csv', row.names=FALSE)
write.csv(x.config.all.shuffled.maressa, 'rcbd.config.maressa.csv', row.names=FALSE)
```

### Execução de configuração
```{r config_execution, eval = FALSE}
# Load packages -----------------------------------------------------------
if (!require(ExpDE, quietly = TRUE)){
  install.packages("ExpDE")
}


if (!require(smoof, quietly = TRUE)){
  install.packages("smoof")
}

# Execute a RCBD test configuration ------------------------------------------

# Define a class to store the levels configuration
level.config <- function(mp, rp, id) {
  value <- list(mutparsX = mp, recparsX = rp, id = id)
  class(value) <- append(class(value),"level.config")
  return(value)
}

## Equipe D
## Config 1
recpars1 <- list(name = "recombination_blxAlphaBeta", alpha = 0.4, beta = 0.4)
mutpars1 <- list(name = "mutation_rand", f = 4)

## Config 2
recpars2 <- list(name = "recombination_eigen", othername = "recombination_bin", cr = 0.9)
mutpars2 <- list(name = "mutation_best", f = 2.8)

config.1 <- level.config(mutpars1, recpars1, 1)
config.2 <- level.config(mutpars2, recpars2, 2)


fname = 'rcbd.config.victor.csv'
Z <- read.csv(fname)
set.seed(15632) # set a random seed

my.ExpDE <- function(mutp, recp, dim){
  
  fn.current <- function(X){
    if(!is.matrix(X)) X <- matrix(X, nrow = 1) # <- if a single vector is passed as Z
    
    Y <- apply(X, MARGIN = 1, FUN = smoof::makeRosenbrockFunction(dimensions = dim))
    return(Y)
  }
  
  
  assign("fn", fn.current, envir = .GlobalEnv)
  
  selpars <- list(name = "selection_standard")
  stopcrit <- list(names = "stop_maxeval", maxevals = 5000 * dim, maxiter = 100 * dim)
  probpars <- list(name = "fn", xmin = rep(-5, dim), xmax = rep(10, dim))
  popsize = 5 * dim
  
  out <- ExpDE(mutpars = mutp,
               recpars = recp,
               popsize = popsize,
               selpars = selpars,
               stopcrit = stopcrit,
               probpars = probpars,
               showpars = list(show.iters = "none"))
  
  return(list(value = out$Fbest))
}

for (row in 1:nrow(Z)){
  
  if(Z[row, "result"] == -1){ # start from the last execution
    dim <- Z[row, "instance"]
    algo <- Z[row, "algorithm"]
    replicate <- Z[row, "replicate"]
    
    if(algo == 1)
      algo.config <- config.1
    else
      algo.config <- config.2

    print(paste("Started Instance:", dim, "; Algo:", algo, "; Repetition:", replicate))
    
    out <- my.ExpDE(algo.config$mutparsX, algo.config$recparsX, dim)
    
    Z[row, "result"] <- out$value
    print(paste("Finished. Instance:", dim, "; Algo:", algo, "; 
                Repetition:", replicate, "; Result=", out$value))
    print(paste("Progress = ", 100 * row / nrow(Z) , "%"))    
    write.csv(Z, fname)
    
  }
}
```


## Referências
